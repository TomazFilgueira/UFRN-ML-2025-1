{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN4sFrPI3SP6pOvC674cSKd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TomazFilgueira/UFRN-ML-2025-1-Heart_Disease_Classfication/blob/main/heart_disease_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Import Libraries"
      ],
      "metadata": {
        "id": "3rEiE_dqAfSh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "from sklearn.datasets import make_moons\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, roc_curve, precision_recall_curve, auc\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "%matplotlib inline\n",
        "plt.style.use('fivethirtyeight')\n"
      ],
      "metadata": {
        "id": "0tA1veEhAiP2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install lazypredict"
      ],
      "metadata": {
        "id": "IGFY-rGr8F-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from lazypredict.Supervised import LazyClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "P57x4d6G8SfR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Generate confusion matrix line figure\n"
      ],
      "metadata": {
        "id": "7u8CgMeMVKyE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def figure10(y, probabilities, threshold, shift, annot, colors=None,title=\"\"):\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
        "    probability_line(ax, y, probabilities, threshold, shift, annot, colors)\n",
        "    ax.set_title(title)\n",
        "    fig.tight_layout()\n",
        "    return fig"
      ],
      "metadata": {
        "id": "iHS3uiLoVO7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def probability_contour(ax, model, device, X, y, threshold, cm=None, cm_bright=None):\n",
        "    if cm is None:\n",
        "        cm = plt.cm.RdBu\n",
        "    if cm_bright is None:\n",
        "        cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
        "\n",
        "    h = .02  # step size in the mesh\n",
        "\n",
        "    x_min, x_max = -2.25, 2.25\n",
        "    y_min, y_max = -2.25, 2.25\n",
        "\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                         np.arange(y_min, y_max, h))\n",
        "\n",
        "    logits = model(torch.as_tensor(np.c_[xx.ravel(), yy.ravel()]).float().to(device))\n",
        "    logits = logits.detach().cpu().numpy().reshape(xx.shape)\n",
        "\n",
        "    yhat = sigmoid(logits)\n",
        "\n",
        "    ax.contour(xx, yy, yhat, levels=[threshold], cmap=\"Greys\", vmin=0, vmax=1)\n",
        "    contour = ax.contourf(xx, yy, yhat, 25, cmap=cm, alpha=.8, vmin=0, vmax=1)\n",
        "    # Plot the training points\n",
        "    ax.scatter(X[:, 0], X[:, 1], c=y, cmap=cm_bright, edgecolors='k')\n",
        "    # Plot the testing points\n",
        "    #ax.scatter(X_val[:, 0], X_val[:, 1], c=y_val, cmap=cm_bright, edgecolors='k', alpha=0.6)\n",
        "\n",
        "    ax.set_xlim(xx.min(), xx.max())\n",
        "    ax.set_ylim(yy.min(), yy.max())\n",
        "    ax.set_xlabel(r'$X_1$')\n",
        "    ax.set_ylabel(r'$X_2$')\n",
        "    ax.set_title(r'$\\sigma(z) = P(y=1)$')\n",
        "    ax.grid(False)\n",
        "\n",
        "    ax_c = plt.colorbar(contour)\n",
        "    ax_c.set_ticks([0, .25, .5, .75, 1])\n",
        "    return ax"
      ],
      "metadata": {
        "id": "KhWQ2cx-YZp7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def probability_line(ax, y, probs, threshold, shift=0.0, annot=False, colors=None):\n",
        "    if colors is None:\n",
        "        colors = ['r', 'b']\n",
        "    ax.grid(False)\n",
        "    ax.set_ylim([-.1, .1])\n",
        "    ax.axes.get_yaxis().set_visible(False)\n",
        "    ax.plot([0, 1], [0, 0], linewidth=2, c='k', zorder=1)\n",
        "    ax.plot([0, 0], [-.1, .1], c='k', zorder=1)\n",
        "    ax.plot([1, 1], [-.1, .1], c='k', zorder=1)\n",
        "\n",
        "    tn = (y == 0) & (probs < threshold)\n",
        "    fn = (y == 0) & (probs >= threshold)\n",
        "    tp = (y == 1) & (probs >= threshold)\n",
        "    fp = (y == 1) & (probs < threshold)\n",
        "\n",
        "    ax.plot([threshold, threshold], [-.1, .1], c='k', zorder=0.5, linestyle='--')\n",
        "    ax.scatter(probs[tn], np.zeros(tn.sum()) + shift, c=colors[0], s=150, zorder=2, edgecolor=colors[0], linewidth=3)\n",
        "    ax.scatter(probs[fn], np.zeros(fn.sum()) + shift, c=colors[0], s=150, zorder=2, edgecolor=colors[1], linewidth=3)\n",
        "\n",
        "    ax.scatter(probs[tp], np.zeros(tp.sum()) - shift, c=colors[1], s=150, zorder=2, edgecolor=colors[1], linewidth=3)\n",
        "    ax.scatter(probs[fp], np.zeros(fp.sum()) - shift, c=colors[1], s=150, zorder=2, edgecolor=colors[0], linewidth=3)\n",
        "\n",
        "    ax.set_xlabel('Threshold = {}'.format(threshold))\n",
        "    #ax.set_title('Threshold = {}'.format(threshold))\n",
        "\n",
        "    if annot:\n",
        "        ax.annotate('TN', xy=(.20, .03), c='k', weight='bold', fontsize=20)\n",
        "        ax.annotate('FN', xy=(.20, -.08), c='k', weight='bold', fontsize=20)\n",
        "        ax.annotate('FP', xy=(.70, .03), c='k', weight='bold', fontsize=20)\n",
        "        ax.annotate('TP', xy=(.70, -.08), c='k', weight='bold', fontsize=20)\n",
        "    return ax"
      ],
      "metadata": {
        "id": "U4SuuiBQYTIq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Architeture Classs\n"
      ],
      "metadata": {
        "id": "XY81QgD4DTSF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Architecture(object):\n",
        "    def __init__(self, model, loss_fn, optimizer):\n",
        "        # Here we define the attributes of our class\n",
        "\n",
        "        # We start by storing the arguments as attributes\n",
        "        # to use them later\n",
        "        self.model = model\n",
        "        self.loss_fn = loss_fn\n",
        "        self.optimizer = optimizer\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        # Let's send the model to the specified device right away\n",
        "        self.model.to(self.device)\n",
        "\n",
        "        # These attributes are defined here, but since they are\n",
        "        # not informed at the moment of creation, we keep them None\n",
        "        self.train_loader = None\n",
        "        self.val_loader = None\n",
        "\n",
        "        # These attributes are going to be computed internally\n",
        "        self.losses = []\n",
        "        self.val_losses = []\n",
        "        self.total_epochs = 0\n",
        "\n",
        "        # Creates the train_step function for our model,\n",
        "        # loss function and optimizer\n",
        "        # Note: there are NO ARGS there! It makes use of the class\n",
        "        # attributes directly\n",
        "        self.train_step_fn = self._make_train_step_fn()\n",
        "        # Creates the val_step function for our model and loss\n",
        "        self.val_step_fn = self._make_val_step_fn()\n",
        "\n",
        "    def to(self, device):\n",
        "        # This method allows the user to specify a different device\n",
        "        # It sets the corresponding attribute (to be used later in\n",
        "        # the mini-batches) and sends the model to the device\n",
        "        try:\n",
        "            self.device = device\n",
        "            self.model.to(self.device)\n",
        "        except RuntimeError:\n",
        "            self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "            print(f\"Couldn't send it to {device}, sending it to {self.device} instead.\")\n",
        "            self.model.to(self.device)\n",
        "\n",
        "    def set_loaders(self, train_loader, val_loader=None):\n",
        "        # This method allows the user to define which train_loader (and val_loader, optionally) to use\n",
        "        # Both loaders are then assigned to attributes of the class\n",
        "        # So they can be referred to later\n",
        "        self.train_loader = train_loader\n",
        "        self.val_loader = val_loader\n",
        "\n",
        "    def _make_train_step_fn(self):\n",
        "        # This method does not need ARGS... it can refer to\n",
        "        # the attributes: self.model, self.loss_fn and self.optimizer\n",
        "\n",
        "        # Builds function that performs a step in the train loop\n",
        "        def perform_train_step_fn(x, y):\n",
        "            # Sets model to TRAIN mode\n",
        "            self.model.train()\n",
        "\n",
        "            # Step 1 - Computes our model's predicted output - forward pass\n",
        "            yhat = self.model(x)\n",
        "            # Step 2 - Computes the loss\n",
        "            loss = self.loss_fn(yhat, y)\n",
        "            # Step 3 - Computes gradients for both \"a\" and \"b\" parameters\n",
        "            loss.backward()\n",
        "            # Step 4 - Updates parameters using gradients and the learning rate\n",
        "            self.optimizer.step()\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "            # Returns the loss\n",
        "            return loss.item()\n",
        "\n",
        "        # Returns the function that will be called inside the train loop\n",
        "        return perform_train_step_fn\n",
        "\n",
        "    def _make_val_step_fn(self):\n",
        "        # Builds function that performs a step in the validation loop\n",
        "        def perform_val_step_fn(x, y):\n",
        "            # Sets model to EVAL mode\n",
        "            self.model.eval()\n",
        "\n",
        "            # Step 1 - Computes our model's predicted output - forward pass\n",
        "            yhat = self.model(x)\n",
        "            # Step 2 - Computes the loss\n",
        "            loss = self.loss_fn(yhat, y)\n",
        "            # There is no need to compute Steps 3 and 4, since we don't update parameters during evaluation\n",
        "            return loss.item()\n",
        "\n",
        "        return perform_val_step_fn\n",
        "\n",
        "    def _mini_batch(self, validation=False):\n",
        "        # The mini-batch can be used with both loaders\n",
        "        # The argument `validation`defines which loader and\n",
        "        # corresponding step function is going to be used\n",
        "        if validation:\n",
        "            data_loader = self.val_loader\n",
        "            step_fn = self.val_step_fn\n",
        "        else:\n",
        "            data_loader = self.train_loader\n",
        "            step_fn = self.train_step_fn\n",
        "\n",
        "        if data_loader is None:\n",
        "            return None\n",
        "\n",
        "        # Once the data loader and step function, this is the same\n",
        "        # mini-batch loop we had before\n",
        "        mini_batch_losses = []\n",
        "        for x_batch, y_batch in data_loader:\n",
        "            x_batch = x_batch.to(self.device)\n",
        "            y_batch = y_batch.to(self.device)\n",
        "\n",
        "            mini_batch_loss = step_fn(x_batch, y_batch)\n",
        "            mini_batch_losses.append(mini_batch_loss)\n",
        "\n",
        "        loss = np.mean(mini_batch_losses)\n",
        "        return loss\n",
        "\n",
        "    def set_seed(self, seed=42):\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "        torch.manual_seed(seed)\n",
        "        np.random.seed(seed)\n",
        "\n",
        "    def train(self, n_epochs, seed=42):\n",
        "        # To ensure reproducibility of the training process\n",
        "        self.set_seed(seed)\n",
        "\n",
        "        for epoch in range(n_epochs):\n",
        "            # Keeps track of the numbers of epochs\n",
        "            # by updating the corresponding attribute\n",
        "            self.total_epochs += 1\n",
        "\n",
        "            # inner loop\n",
        "            # Performs training using mini-batches\n",
        "            loss = self._mini_batch(validation=False)\n",
        "            self.losses.append(loss)\n",
        "\n",
        "            # VALIDATION\n",
        "            # no gradients in validation!\n",
        "            with torch.no_grad():\n",
        "                # Performs evaluation using mini-batches\n",
        "                val_loss = self._mini_batch(validation=True)\n",
        "                self.val_losses.append(val_loss)\n",
        "\n",
        "    def save_checkpoint(self, filename):\n",
        "        # Builds dictionary with all elements for resuming training\n",
        "        checkpoint = {'epoch': self.total_epochs,\n",
        "                      'model_state_dict': self.model.state_dict(),\n",
        "                      'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "                      'loss': self.losses,\n",
        "                      'val_loss': self.val_losses}\n",
        "\n",
        "        torch.save(checkpoint, filename)\n",
        "\n",
        "    def load_checkpoint(self, filename):\n",
        "        # Loads dictionary\n",
        "        checkpoint = torch.load(filename,weights_only=False)\n",
        "\n",
        "        # Restore state for model and optimizer\n",
        "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "        self.total_epochs = checkpoint['epoch']\n",
        "        self.losses = checkpoint['loss']\n",
        "        self.val_losses = checkpoint['val_loss']\n",
        "\n",
        "        self.model.train() # always use TRAIN for resuming training\n",
        "\n",
        "    def predict(self, x):\n",
        "        # Set is to evaluation mode for predictions\n",
        "        self.model.eval()\n",
        "        # Takes aNumpy input and make it a float tensor\n",
        "        x_tensor = torch.as_tensor(x).float()\n",
        "        # Send input to device and uses model for prediction\n",
        "        y_hat_tensor = self.model(x_tensor.to(self.device))\n",
        "        # Set it back to train mode\n",
        "        self.model.train()\n",
        "        # Detaches it, brings it to CPU and back to Numpy\n",
        "        return y_hat_tensor.detach().cpu().numpy()\n",
        "\n",
        "    def plot_losses(self,title=\"\"):\n",
        "        fig = plt.figure(figsize=(10, 4))\n",
        "        plt.plot(self.losses, label='Training Loss', c='b')\n",
        "        plt.plot(self.val_losses, label='Validation Loss', c='r')\n",
        "        plt.title(title)\n",
        "        plt.yscale('log')\n",
        "        plt.xlabel('Epochs')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "        plt.tight_layout()\n",
        "        return fig"
      ],
      "metadata": {
        "id": "g6klcDiUDQKL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Read Reduced CSV File from Github"
      ],
      "metadata": {
        "id": "koq2OBME7Hup"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_MyTkbTT7Dc4"
      },
      "outputs": [],
      "source": [
        "url = \"https://raw.githubusercontent.com/TomazFilgueira/UFRN-ML-2025-1-Heart_Disease_Classfication/main/0_data/reduced_heart_disease_dataset.csv\" # Correct URL to raw file content\n",
        "\n",
        "try:\n",
        "  df = pd.read_csv(url)\n",
        "  print(\"Successfully read CSV from GitHub\")\n",
        "  # Now you can work with the DataFrame 'df'\n",
        "  print(df.head()) # Example: Display the first few rows\n",
        "\n",
        "except Exception as e:\n",
        "  print(f\"An error occurred: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Checkpoints\n",
        "\n",
        "After this points we will pass for a series of checkpoint in order to get our dataset classified using pytorch.\n",
        "\n",
        "Those checkpoints are:\n",
        "\n",
        "1.  Data Preparation:\n",
        "  * Create dummy variables\n",
        "  * Creator tensors\n",
        "  * build train and validation dataset/dataloader\n",
        "\n",
        "2. Configure Model: determine some parameters:\n",
        "  * Which model should we use to classify binary output?\n",
        "  * Defines Stocastic Gradient Descent\n",
        "  * Defines a loss function to classification\n",
        "\n",
        "3. Train the model itself using `Architeture()` class\n",
        "\n",
        "4. Validate the Model:\n",
        "  * is the model accurate to our problem?\n",
        "  * Let's evaluate some metrics such as:\n",
        "    - Recall/Precision\n",
        "    - Accuracy\n",
        "    - True and False Positive Rates\n",
        "\n",
        "4. Make Predictions"
      ],
      "metadata": {
        "id": "0IDMXpcFPC7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1) Data Preparation\n",
        "\n",
        "Most of work has been done during the EDA parts. However, something must be adapted in order to our classify works properly\n",
        "\n",
        "First we need to convert categorical features into dummy ones."
      ],
      "metadata": {
        "id": "9kW2Kla1F2el"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "ahWJKZekGiBB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dummy variables for categorical features\n",
        "categorical_cols = ['sex','cp', 'restecg', 'exang', 'slope','thal','elderly']\n",
        "df_dummy = pd.get_dummies(df, columns=categorical_cols, drop_first=True)\n",
        "print(df_dummy.shape)"
      ],
      "metadata": {
        "id": "FbWZmNTOGKUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Balanced target column\n",
        "One of the biggest mistakes in classification problem is to let train and validation dataset with disproportional values of our target column.\n",
        "\n",
        "because of that we will calculate the proportion of target in the main dataset and will leave with same ratio in train and validation division.\n"
      ],
      "metadata": {
        "id": "nhjaU237IjWX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate target proportion\n",
        "target_proportion = df['target'].value_counts(normalize=True)\n",
        "target_proportion\n"
      ],
      "metadata": {
        "id": "HLdvZfvCJDYG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Creating tensors and building train/validation dataset"
      ],
      "metadata": {
        "id": "4O7LsmBHKH1x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define features (X) and target (y)\n",
        "X = df_dummy.drop('target', axis=1).values\n",
        "y = df_dummy['target'].values\n",
        "\n",
        "# Split data into training and validation sets while maintaining target proportion\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=df_dummy['target'])\n",
        "\n",
        "# Scale features using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).reshape(-1, 1)  # Reshape for single output\n",
        "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
        "y_val_tensor = torch.tensor(y_val, dtype=torch.float32).reshape(-1, 1)\n",
        "\n",
        "# Create TensorDatasets\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
        "\n",
        "# Create DataLoaders in mini_batch type with 16 observations\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)"
      ],
      "metadata": {
        "id": "38y8RDwkGYG-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the proportion of True values in y_train\n",
        "true_proportion_train = np.sum(y_train) / len(y_train)\n",
        "\n",
        "# Calculate the proportion of True values in y_val\n",
        "true_proportion_val = np.sum(y_val) / len(y_val)\n",
        "\n",
        "print(f\"Proportion of True values in y_train: {true_proportion_train}\")\n",
        "print(f\"Proportion of True values in y_val: {true_proportion_val}\")\n"
      ],
      "metadata": {
        "id": "MwjZx5i_J4ei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code above is just to be sure that our stratification in target column is made correctly.\n",
        "\n",
        "The proportion of true values in train and validation dataset is the same of the original dataframe"
      ],
      "metadata": {
        "id": "VTFBkeBcKZA2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2) Configure Model\n",
        "\n",
        "In this section we will configure our classification model.\n",
        "\n",
        "Starting with the following hyper-parameters:\n",
        "* lr = 0.05\n",
        "* model: linear with 19 predictors (dataset features exluding target column)\n",
        "* Optimizer: Stochastic Gradient Descent\n",
        "* Loss Function: Binary-Cross Entropy with Logit Loss. This loss function is widely used in classification problems because its output means logit that can be used for determining a probability of an event to happen\n"
      ],
      "metadata": {
        "id": "lhVbNGM5Km6h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sets learning rate - this is \"eta\" ~ the \"n\" like Greek letter\n",
        "lr = 0.05\n",
        "\n",
        "torch.manual_seed(42)\n",
        "model = nn.Sequential()\n",
        "model.add_module('linear', nn.Linear(19, 1))\n",
        "\n",
        "# Defines a SGD optimizer to update the parameters\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
        "\n",
        "# Defines a BCE loss function\n",
        "loss_fn = nn.BCEWithLogitsLoss()"
      ],
      "metadata": {
        "id": "WDf9QpoxKt7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3 and 4)Training/Validation"
      ],
      "metadata": {
        "id": "KfakHYMYMCkT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#set number of epochs\n",
        "n_epochs = 200\n",
        "\n",
        "#using Architecture class passing model, loss and optimized as parameters\n",
        "arch = Architecture(model, loss_fn, optimizer)\n",
        "arch.set_loaders(train_loader, val_loader)\n",
        "arch.set_seed(42)\n",
        "arch.train(n_epochs)\n"
      ],
      "metadata": {
        "id": "CL6embDdMG5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = arch.plot_losses()"
      ],
      "metadata": {
        "id": "brXlLttUNIpU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the figure above we can see the lines of training and validation loss.\n",
        "\n",
        "It is ease to see that our model did not overfitted because the error of train/validation remained similar throughout the epochs. However, the validation error itself can be optimized."
      ],
      "metadata": {
        "id": "3M7BEDB2OaWp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.state_dict())"
      ],
      "metadata": {
        "id": "h1AhAeApNjDD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Metrics\n"
      ],
      "metadata": {
        "id": "Nk0AkE2bO80F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_cm(cm):\n",
        "    # Actual negatives go in the top row,\n",
        "    # above the probability line\n",
        "    actual_negative = cm[0]\n",
        "    # Predicted negatives go in the first column\n",
        "    tn = actual_negative[0]\n",
        "    # Predicted positives go in the second column\n",
        "    fp = actual_negative[1]\n",
        "\n",
        "    # Actual positives go in the bottow row,\n",
        "    # below the probability line\n",
        "    actual_positive = cm[1]\n",
        "    # Predicted negatives go in the first column\n",
        "    fn = actual_positive[0]\n",
        "    # Predicted positives go in the second column\n",
        "    tp = actual_positive[1]\n",
        "\n",
        "    return tn, fp, fn, tp"
      ],
      "metadata": {
        "id": "EGL8qNQiZt27"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#From logit to probablities\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "\n",
        "logits_val = arch.predict(X_val)\n",
        "probabilities_val = sigmoid(logits_val).squeeze()\n",
        "threshold = 0.5\n",
        "\n",
        "#Confusion Matrix using SKlearn\n",
        "cm_model1 = confusion_matrix(y_val, (probabilities_val >= threshold))\n",
        "cm_model1"
      ],
      "metadata": {
        "id": "eLBeE1mKPXDd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Confusion Matrix from Validation dataset\n",
        "fig = figure10(y_val, probabilities_val, threshold, 0.05, True,title=\"Confusion Matrix for Model 1\")"
      ],
      "metadata": {
        "id": "9wupMr97ZKJT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##True and False Positive Rates\n",
        "$$\n",
        "\\Large \\text{TPR} = \\frac{\\text{TP}}{\\text{TP + FN}} \\ \\ \\  \\text{FPR} = \\frac{\\text{FP}}{\\text{FP + TN}}\n",
        "$$"
      ],
      "metadata": {
        "id": "lj2r9Lo5Z0nF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tpr_fpr(cm):\n",
        "    tn, fp, fn, tp = split_cm(cm)\n",
        "\n",
        "    tpr = tp / (tp + fn)\n",
        "    fpr = fp / (fp + tn)\n",
        "\n",
        "    return tpr, fpr\n"
      ],
      "metadata": {
        "id": "Wi8bkrYFaGTR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Precision and Recall\n",
        "\n",
        "$$\n",
        "\\Large \\text{Recall} = \\frac{\\text{TP}}{\\text{TP + FN}} \\ \\ \\  \\text{Precision} = \\frac{\\text{TP}}{\\text{TP + FP}}\n",
        "$$"
      ],
      "metadata": {
        "id": "dLQc5GxAbAmW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def precision_recall(cm):\n",
        "    tn, fp, fn, tp = split_cm(cm)\n",
        "\n",
        "    precision = tp / (tp + fp)\n",
        "    recall = tp / (tp + fn)\n",
        "\n",
        "    return precision, recall"
      ],
      "metadata": {
        "id": "f6FlBr35bFO3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Accuracy\n",
        "\n",
        "$$\n",
        "\\Large \\text{Accuracy} = \\frac{\\text{TP+TN}}{\\text{TP+TN+FP+FN}}\n",
        "$$\n",
        "\n",
        "We can use `accuracy_score()` method directly from Sklearn"
      ],
      "metadata": {
        "id": "NuLFySgXbKi4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Evaluation"
      ],
      "metadata": {
        "id": "pwBITWbJbjCg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Precision-Recall\n",
        "precision,recall = precision_recall(cm_model1)\n",
        "#Accuracy\n",
        "acc = accuracy_score(y_val, (probabilities_val >= threshold))\n",
        "#True and False Positive Rates\n",
        "tpr,fpr = tpr_fpr(cm_model1)\n",
        "\n",
        "print(f\"Precision Metrics: {precision}\\n\")\n",
        "print(f\"Recall Metrics: {recall}\\n\")\n",
        "print(f\"Accuracy score {acc}\\n\")\n",
        "print(f\"True Positive Rate:{tpr}\\n\")\n",
        "print(f\"False Positive Rate:{fpr}\\n\")"
      ],
      "metadata": {
        "id": "qrO1ifdhbNRa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_metrics(precision, recall, acc, tpr, fpr):\n",
        "    # Create a figure and a 1x3 grid of subplots\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "    # Subplot 1: Precision and Recall\n",
        "    axes[0].bar(['Precision', 'Recall'], [precision, recall], color=['skyblue', 'lightcoral'])\n",
        "    axes[0].set_title(\"Precision and Recall\")\n",
        "    axes[0].set_ylim(0, 1.1)\n",
        "    axes[0].grid(visible=None)\n",
        "    axes[0].set_yticks([])  # Hide y-axis values for subplot 1\n",
        "\n",
        "    for i, v in enumerate([precision, recall]):\n",
        "        axes[0].text(i, v + 0.02, f\"{v:.4f}\", ha='center', va='bottom', fontsize=15)\n",
        "\n",
        "    # Subplot 2: TPR and FPR\n",
        "    axes[1].bar(['TPR', 'FPR'], [tpr, fpr], color=['lightgreen', 'gold'])\n",
        "    axes[1].set_title(\"TPR and FPR\")\n",
        "    axes[1].set_ylim(0, 1.1)\n",
        "    axes[1].grid(visible=None)\n",
        "    axes[1].set_yticks([])  # Hide y-axis values for subplot 2\n",
        "\n",
        "    for i, v in enumerate([tpr, fpr]):\n",
        "        axes[1].text(i, v + 0.02, f\"{v:.4f}\", ha='center', va='bottom', fontsize=15)\n",
        "\n",
        "    # Subplot 3: Accuracy\n",
        "    axes[2].bar('Accuracy', acc, color=['lightblue'])\n",
        "    axes[2].set_title(\"Accuracy\")\n",
        "    axes[2].set_ylim(0, 1.1)\n",
        "    axes[2].text(0, acc + 0.02, f\"{acc:.4f}\", ha='center', va='bottom', fontsize=15)\n",
        "    axes[2].grid(visible=None)\n",
        "    axes[2].set_yticks([])  # Hide y-axis values for subplot 3\n",
        "\n",
        "    fig.tight_layout()  # Adjust subplot parameters for a tight layout\n",
        "    plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "1DK2KNLKchpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Calling the plot metrics\n",
        "plot_metrics(precision, recall, acc, tpr, fpr)"
      ],
      "metadata": {
        "id": "CvgDZsibZpkY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Conclusion\n",
        "The first model was configurated using these parameters below:\n",
        "\n",
        "  * lr = 0.05\n",
        "  * train/val split ratio = 0.2\n",
        "  * number of epochs = 200\n",
        "  * Optimizer: SGD\n",
        "\n",
        "With this configuration we got the results from the confusion matrix and bar graph above:\n",
        "\n",
        "  1. Precision Metrics: 0.75\n",
        "\n",
        "  1. Recall Metrics: 0.833\n",
        "\n",
        "  1. Accuracy score 0.812\n",
        "\n",
        "  1. True Positive Rate:0.833\n",
        "\n",
        "  1. False Positive Rate:0.204\n",
        "\n",
        "More over, knowing that Heart disease identification is a sensible matter we have to bear in mind the **False Negative** number which means that an individual has a heart problem and our model did not identified properly, leaving the person in serious risk!\n",
        "\n",
        "For model 1 and using a threshold limiter of 0.5 we got False Negative Number of 6 misclassication.\n",
        "\n",
        "It seems low, right? but even lower the better. Can we decrease even more this number?\n",
        "\n",
        "From mode 1 we have used the whole heart desease dataset. However, during our EDA phase it has been identified that some feature can iteract more with our target column than the other. Let's filter our dataset with this \"best features\""
      ],
      "metadata": {
        "id": "lk0tOR-gk68n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Read selected feature CSV\n",
        "\n",
        "From now on we will be using the \"selected\" dataset. Here will be using numerical features with correlation greater than 0.2 with compared to target column. Besides we will be still using all categorical features as presented before.\n",
        "\n",
        "The features of this dataset are:\n",
        "\n",
        "**categorical_cols**\n",
        "\n",
        " - sex\n",
        " - cp\n",
        " - restecg\n",
        " - exang\n",
        " - slope\n",
        " - thal\n",
        " - elderly\n",
        "\n",
        "**Numerical**\n",
        " - Oldpeak\n",
        " - Thalachh"
      ],
      "metadata": {
        "id": "XvtwbQNqRX6p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://raw.githubusercontent.com/TomazFilgueira/UFRN-ML-2025-1-Heart_Disease_Classfication/main/0_data/selected_features_dataset.csv\" # Correct URL to raw file content\n",
        "\n",
        "try:\n",
        "  df_selected = pd.read_csv(url)\n",
        "  print(\"Successfully read CSV from GitHub\")\n",
        "  # Now you can work with the DataFrame 'df'\n",
        "  print(df.head()) # Example: Display the first few rows\n",
        "\n",
        "except Exception as e:\n",
        "  print(f\"An error occurred: {e}\")"
      ],
      "metadata": {
        "id": "_hSRPSrhQqMl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dummy variables for categorical features\n",
        "categorical_cols = ['sex','cp', 'restecg', 'exang', 'slope','thal','elderly']\n",
        "df_selected_dummy = pd.get_dummies(df_selected, columns=categorical_cols, drop_first=True)\n",
        "print(df_dummy.shape)"
      ],
      "metadata": {
        "id": "ke10ZkqPRhV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define features (X) and target (y)\n",
        "X_model2 = df_selected_dummy.drop('target', axis=1).values\n",
        "y_model2 = df_selected_dummy['target'].values\n",
        "\n",
        "# Split data into training and validation sets while maintaining target proportion\n",
        "X_train_model2, X_val_model2, y_train_model2, y_val_model2 = train_test_split(X_model2, y_model2, test_size=0.2, random_state=42, stratify=df_selected_dummy['target'])\n",
        "\n",
        "# Scale features using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train_model2 = scaler.fit_transform(X_train_model2)\n",
        "X_val_model2 = scaler.transform(X_val_model2)\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "X_train_tensor_model2 = torch.tensor(X_train_model2, dtype=torch.float32)\n",
        "y_train_tensor_model2 = torch.tensor(y_train_model2, dtype=torch.float32).reshape(-1, 1)  # Reshape for single output\n",
        "X_val_tensor_model2 = torch.tensor(X_val_model2, dtype=torch.float32)\n",
        "y_val_tensor_model2 = torch.tensor(y_val_model2, dtype=torch.float32).reshape(-1, 1)\n",
        "\n",
        "# Create TensorDatasets\n",
        "train_dataset_model2 = TensorDataset(X_train_tensor_model2, y_train_tensor_model2)\n",
        "val_dataset_model2 = TensorDataset(X_val_tensor_model2, y_val_tensor_model2)\n",
        "\n",
        "# Create DataLoaders in mini_batch type with 16 observations\n",
        "train_loader_model2 = DataLoader(train_dataset_model2, batch_size=16, shuffle=True)\n",
        "val_loader_model2 = DataLoader(val_dataset_model2, batch_size=16, shuffle=False)"
      ],
      "metadata": {
        "id": "bugNQgniRxRT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Configure second model"
      ],
      "metadata": {
        "id": "0BFPjIXmR8LG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sets learning rate - this is \"eta\" ~ the \"n\" like Greek letter\n",
        "lr = 0.05\n",
        "\n",
        "torch.manual_seed(42)\n",
        "model2 = nn.Sequential()\n",
        "#This model contains 16 input instead of 19 of model 1\n",
        "model2.add_module('linear', nn.Linear(16, 1))\n",
        "\n",
        "# Defines a SGD optimizer to update the parameters\n",
        "optimizer2 = optim.SGD(model2.parameters(), lr=lr)\n",
        "\n",
        "# Defines a BCE loss function\n",
        "loss_fn = nn.BCEWithLogitsLoss()"
      ],
      "metadata": {
        "id": "Ky1XxzeTR-QA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train second model"
      ],
      "metadata": {
        "id": "6NIl8wfOSPrw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#set number of epochs\n",
        "n_epochs = 200\n",
        "\n",
        "#using Architecture class passing model, loss and optimized as parameters\n",
        "arch2 = Architecture(model2, loss_fn, optimizer2)\n",
        "arch2.set_loaders(train_loader_model2, val_loader_model2)\n",
        "arch2.set_seed(42)\n",
        "arch2.train(n_epochs)\n"
      ],
      "metadata": {
        "id": "vje-IBhISRKh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = arch2.plot_losses()"
      ],
      "metadata": {
        "id": "nuvhNPYxSlcn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Metrics for second model\n"
      ],
      "metadata": {
        "id": "3_gF4PT7UT21"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logits_val = arch2.predict(X_val_model2)\n",
        "probabilities_model2_val = sigmoid(logits_val).squeeze()\n",
        "threshold = 0.5\n",
        "#Confusion Matrix using SKlearn\n",
        "cm_model2 = confusion_matrix(y_val_model2, (probabilities_model2_val >= threshold))\n",
        "cm_model2"
      ],
      "metadata": {
        "id": "3IuBeD9qUiuH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Confusion Matrix from Validation dataset\n",
        "fig = figure10(y_val_model2, probabilities_model2_val, threshold, 0.05, True,title=\"Confusion Matrix for Model 2\")"
      ],
      "metadata": {
        "id": "WTUZ1CvDVjo5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Precision-Recall\n",
        "precision2,recall2 = precision_recall(cm_model2)\n",
        "#Accuracy\n",
        "acc2 = accuracy_score(y_val_model2, (probabilities_model2_val >= threshold))\n",
        "#True and False Positive Rates\n",
        "tpr2,fpr2 = tpr_fpr(cm_model2)\n",
        "\n",
        "print(f\"Precision Metrics: {precision2}\\n\")\n",
        "print(f\"Recall Metrics: {recall2}\\n\")\n",
        "print(f\"Accuracy score {acc2}\\n\")\n",
        "print(f\"True Positive Rate:{tpr2}\\n\")\n",
        "print(f\"False Positive Rate:{fpr2}\\n\")"
      ],
      "metadata": {
        "id": "HhonJZNSUq3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Calling the plot metrics\n",
        "plot_metrics(precision2, recall2, acc2, tpr2, fpr2)"
      ],
      "metadata": {
        "id": "6u41HWTMVFhZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Comparing models\n",
        "metrics_model1 = {'Precision': precision, 'Recall': recall, 'Accuracy': acc, 'TPR': tpr, 'FPR': fpr}\n",
        "metrics_model2 = {'Precision': precision2, 'Recall': recall2, 'Accuracy': acc2, 'TPR': tpr2, 'FPR': fpr2}\n",
        "\n",
        "metrics = ['Precision', 'Recall', 'Accuracy', 'TPR', 'FPR']\n",
        "\n",
        "# Calculate percentage increase\n",
        "percentage_increase = {}\n",
        "for metric in metrics:\n",
        "    increase = ((metrics_model2[metric] - metrics_model1[metric]) / metrics_model1[metric]) * 100\n",
        "    percentage_increase[metric] = increase\n",
        "\n",
        "# Create bar plot\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.bar(metrics, percentage_increase.values(), color=['skyblue', 'lightcoral', 'lightgreen', 'gold', 'lightblue'])\n",
        "plt.xlabel(\"Metrics\")\n",
        "plt.title(\"Percentage Increase in Metrics (Model 2 vs. Model 1)\", pad=30)\n",
        "plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better readability\n",
        "plt.grid(visible=None) #Grid line off\n",
        "plt.yticks([])  #Hide y label\n",
        "\n",
        "\n",
        "# Add percentage values on top of bars\n",
        "for i, v in enumerate(percentage_increase.values()):\n",
        "    plt.text(i, v + 0.1, f\"{v:.2f}%\", ha='center', va='bottom', fontsize=10)\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "dAgeq_faktuw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Conclusion Model 2\n",
        "\n",
        "We can see that using the same configuration of model 1 but using selected feature we had a little bit increase in all metrics except in False Positive Ratio.\n",
        "\n",
        "Here is a summary of the changes:\n",
        "\n",
        "- `Precision` : 1,59% increase. Reaching 0.7619\n",
        "- `Recall` : 6,67% increase. Reaching 0.8889\n",
        "- `Accuracy` : 2,90% increase. Reaching 0.8353\n",
        "- `True Positive Ratio` : 6,67%. Reaching 0.8889\n",
        "- `False Positive Ratio` did not have any change\n",
        "\n",
        "Moreover, when analyzing the False Negative Number it decrease from **6** people in first model to **4** in model 2. It is a satisfatory model!\n",
        "\n",
        "However we can decrease even more this number if we see the confusion matrix from model 2.\n",
        "\n",
        "\n",
        " What if we reduce the threshold value from 0.5 to 0.4?"
      ],
      "metadata": {
        "id": "JA0Uc87Fj1Ag"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Comparing models with threshold 0.4"
      ],
      "metadata": {
        "id": "SirVMAukoTrR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Confusion Matrix - Model 1"
      ],
      "metadata": {
        "id": "VFtKVR44oYvj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logits_val = arch.predict(X_val)\n",
        "probabilities_val = sigmoid(logits_val).squeeze()\n",
        "threshold = 0.4\n",
        "\n",
        "#Confusion Matrix using SKlearn\n",
        "cm_model1_04 = confusion_matrix(y_val, (probabilities_val >= threshold))\n",
        "cm_model1"
      ],
      "metadata": {
        "id": "PBleSUr7ocGY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Confusion Matrix from Validation dataset\n",
        "fig = figure10(y_val, probabilities_val, threshold, 0.05, True,title=\"Confusion Matrix for Model 1\")"
      ],
      "metadata": {
        "id": "8CKvEpfCoy1C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Precision-Recall\n",
        "precision04,recall04 = precision_recall(cm_model1_04)\n",
        "#Accuracy\n",
        "acc04 = accuracy_score(y_val, (probabilities_val >= threshold))\n",
        "#True and False Positive Rates\n",
        "tpr04,fpr04 = tpr_fpr(cm_model1_04)\n",
        "\n",
        "print(f\"Precision Metrics: {precision04}\\n\")\n",
        "print(f\"Recall Metrics: {recall04}\\n\")\n",
        "print(f\"Accuracy score {acc04}\\n\")\n",
        "print(f\"True Positive Rate:{tpr04}\\n\")\n",
        "print(f\"False Positive Rate:{fpr04}\\n\")\n",
        "\n",
        "#Calling the plot metrics\n",
        "plot_metrics(precision04, recall04, acc04, tpr04, fpr04)"
      ],
      "metadata": {
        "id": "tkhGIakEqXhn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Confusion Matrix - Model 2"
      ],
      "metadata": {
        "id": "xc6Ava3bo8t5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logits_val = arch2.predict(X_val_model2)\n",
        "probabilities_model2_val = sigmoid(logits_val).squeeze()\n",
        "threshold = 0.4\n",
        "#Confusion Matrix using SKlearn\n",
        "cm_model2_04 = confusion_matrix(y_val_model2, (probabilities_model2_val >= threshold))\n",
        "cm_model2"
      ],
      "metadata": {
        "id": "-k-pypz1o-9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Confusion Matrix from Validation dataset\n",
        "fig = figure10(y_val_model2, probabilities_model2_val, threshold, 0.05, True,title=\"Confusion Matrix for Model 2\")"
      ],
      "metadata": {
        "id": "2CKqBJIHpHqj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Precision-Recall\n",
        "precision2_04,recall2_04= precision_recall(cm_model2_04)\n",
        "#Accuracy\n",
        "acc2_04 = accuracy_score(y_val_model2, (probabilities_model2_val >= threshold))\n",
        "#True and False Positive Rates\n",
        "tpr2_04,fpr2_04 = tpr_fpr(cm_model2_04)\n",
        "\n",
        "print(f\"Precision Metrics: {precision2_04}\\n\")\n",
        "print(f\"Recall Metrics: {recall2_04}\\n\")\n",
        "print(f\"Accuracy score {acc2_04}\\n\")\n",
        "print(f\"True Positive Rate:{tpr2_04}\\n\")\n",
        "print(f\"False Positive Rate:{fpr2_04}\\n\")\n",
        "\n",
        "#Calling the plot metrics\n",
        "plot_metrics(precision04, recall04, acc04, tpr04, fpr04)"
      ],
      "metadata": {
        "id": "RX8-t8ABpuO3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Project Conclusion\n",
        "After comparing models with 0.4 threshold we could see that both behave similarly. In other word, both models give the same metrics.\n",
        "\n",
        "Both models gives us the following output during the validation set:\n",
        "\n",
        "- `Precision` :  0.7727\n",
        "- `Recall` :  0.9444\n",
        "- `Accuracy` :  0.8588\n",
        "- `True Positive Ratio` : 0.9444\n",
        "- `False Positive Ratio`: 0.2041\n",
        "- `False negative number`: 2 individuals\n",
        "\n",
        "\n",
        "\n",
        "However the second model tends to be better because it has less features when compared to the first one. Hence we will choose the **model 2 to put in production**"
      ],
      "metadata": {
        "id": "rvkHYM9wrtK0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Miscelaneous\n"
      ],
      "metadata": {
        "id": "X7uX2I7H-egT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clf = LazyClassifier(verbose=0, ignore_warnings=True, custom_metric=None)\n",
        "models, predictions = clf.fit(X_train, X_val, y_train, y_val)\n",
        "\n",
        "print(models)"
      ],
      "metadata": {
        "id": "q777sz67-owF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches  # Import patches for highlighting\n",
        "\n",
        "def plot_lazy_model_metrics(models):\n",
        "    \"\"\"Plots bar graph of metrics from LazyClassifier results.\n",
        "\n",
        "    Args:\n",
        "        models: DataFrame of model metrics from LazyClassifier.fit().\n",
        "    \"\"\"\n",
        "\n",
        "    metrics = ['Accuracy', 'Balanced Accuracy', 'ROC AUC']\n",
        "    fig, axes = plt.subplots(len(metrics), 1, figsize=(10, 6 * len(metrics)))\n",
        "\n",
        "    for i, metric in enumerate(metrics):\n",
        "        ax = axes[i]  # Get the current subplot axis\n",
        "        models.sort_values(by=metric, ascending=False).plot(kind='bar', y=metric, ax=ax)\n",
        "        ax.set_title(f\"Models Performance - {metric}\")\n",
        "        ax.set_xlabel(\"\")\n",
        "        ax.set_ylabel(metric)\n",
        "        ax.tick_params(axis='x', rotation=90, labelsize=8)\n",
        "        ax.grid(visible=None)\n",
        "        ax.legend().remove()\n",
        "\n",
        "        # Highlight Logistic Regression bar\n",
        "        if metric in models.columns:  # Check if metric exists in DataFrame\n",
        "            try:\n",
        "                # Find the x-coordinate of Logistic Regression bar\n",
        "                x_coord = models.index.get_loc('LogisticRegression')\n",
        "\n",
        "                # Create a rectangle patch for highlighting\n",
        "                rect = patches.Rectangle((x_coord - 0.4, 0.009), 0.8, models.loc['LogisticRegression', metric],\n",
        "                                        linewidth=2, edgecolor='red', facecolor='none')\n",
        "\n",
        "                # Add the rectangle patch to the subplot\n",
        "                ax.add_patch(rect)\n",
        "            except KeyError:\n",
        "                pass  # Handle case where LogisticRegression is not in models\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_lazy_model_metrics(models)"
      ],
      "metadata": {
        "id": "SzmqmYaN_a11"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}